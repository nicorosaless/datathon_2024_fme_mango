{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, save_img\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "input_dir = '../datathon-fme-mango/archive/images/images'\n",
    "clean_dir = './cleaned_images'\n",
    "resized_dir = './resized_images'\n",
    "\n",
    "# Create directories for cleaned and resized images\n",
    "os.makedirs(clean_dir, exist_ok=True)\n",
    "os.makedirs(resized_dir, exist_ok=True)\n",
    "\n",
    "# Initialize counters\n",
    "corrupted_count = 0\n",
    "cleaned_count = 0\n",
    "resized_count = 0\n",
    "\n",
    "# Clean and resize the dataset\n",
    "print(\"Cleaning and resizing images...\")\n",
    "\n",
    "for item in tqdm(os.listdir(input_dir), desc=\"Processing images\"):\n",
    "    item_path = os.path.join(input_dir, item)\n",
    "    \n",
    "    # Check if the item is a file and has a valid image extension\n",
    "    if os.path.isfile(item_path) and item_path.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "        try:\n",
    "            # Attempt to load the image\n",
    "            img = load_img(item_path)\n",
    "\n",
    "            # If successful, move the image to the cleaned directory\n",
    "            clean_path = os.path.join(clean_dir, item)\n",
    "            os.rename(item_path, clean_path)\n",
    "            cleaned_count += 1\n",
    "\n",
    "            # Resize the image to 224x224\n",
    "            img_resized = img.resize((224, 224))\n",
    "            img_array = img_to_array(img_resized) / 255.0  # Normalize pixel values to 0-1\n",
    "\n",
    "            # Save the resized image to the resized directory\n",
    "            resized_path = os.path.join(resized_dir, item)\n",
    "            save_img(resized_path, img_array)\n",
    "            resized_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            corrupted_count += 1\n",
    "            print(f\"Corrupted image detected and skipped: {item_path} - Error: {e}\")\n",
    "    else:\n",
    "        print(f\"Skipping non-image file: {item_path}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nCleaning and resizing complete.\")\n",
    "print(f\"Total images processed: {cleaned_count + corrupted_count}\")\n",
    "print(f\"Valid images cleaned: {cleaned_count}\")\n",
    "print(f\"Corrupted images removed: {corrupted_count}\")\n",
    "print(f\"Images resized: {resized_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ Resize images (to 224x224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "RESIZED_DIR = './resized_images'\n",
    "OUTPUT_EMBEDDINGS_PATH = './image_embeddings.npy'\n",
    "OUTPUT_FILENAMES_PATH = './image_filenames.npy'\n",
    "OUTPUT_CSV_PATH = './image_embeddings.csv'\n",
    "\n",
    "# Parameters\n",
    "TARGET_SIZE = (224, 224)  # ResNet50 requires 224x224 images\n",
    "BATCH_SIZE = 1000  # Process images in batches to save memory\n",
    "\n",
    "# Load the ResNet50 model\n",
    "print(\"Loading pre-trained ResNet50 model...\")\n",
    "model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "# List all valid image files in the resized directory\n",
    "print(f\"Scanning directory: {RESIZED_DIR}\")\n",
    "image_files = [f for f in os.listdir(RESIZED_DIR) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "total_images = len(image_files)\n",
    "print(f\"Total valid images found: {total_images}\")\n",
    "\n",
    "# Initialize storage for embeddings and filenames\n",
    "all_embeddings = []\n",
    "all_filenames = []\n",
    "\n",
    "# Batch processing\n",
    "print(\"Starting embedding extraction...\")\n",
    "for i in tqdm(range(0, total_images, BATCH_SIZE), desc=\"Processing batches\"):\n",
    "    batch_files = image_files[i:i + BATCH_SIZE]\n",
    "    batch_images = []\n",
    "    valid_files = []\n",
    "\n",
    "    # Load and preprocess images in the batch\n",
    "    for filename in batch_files:\n",
    "        image_path = os.path.join(RESIZED_DIR, filename)\n",
    "        try:\n",
    "            img = load_img(image_path, target_size=TARGET_SIZE)\n",
    "            img_array = img_to_array(img)\n",
    "            img_array = preprocess_input(img_array)\n",
    "            batch_images.append(img_array)\n",
    "            valid_files.append(filename)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "    if batch_images:\n",
    "        # Convert batch to NumPy array and extract embeddings\n",
    "        batch_images_np = np.array(batch_images)\n",
    "        batch_embeddings = model.predict(batch_images_np)\n",
    "        all_embeddings.append(batch_embeddings)\n",
    "        all_filenames.extend(valid_files)\n",
    "\n",
    "# Concatenate all batches into a single array\n",
    "all_embeddings = np.vstack(all_embeddings)\n",
    "print(f\"Embeddings shape: {all_embeddings.shape}\")\n",
    "print(f\"Total filenames recorded: {len(all_filenames)}\")\n",
    "\n",
    "# Save embeddings and filenames to .npy files\n",
    "np.save(OUTPUT_EMBEDDINGS_PATH, all_embeddings)\n",
    "np.save(OUTPUT_FILENAMES_PATH, np.array(all_filenames))\n",
    "print(f\"Saved embeddings to {OUTPUT_EMBEDDINGS_PATH}\")\n",
    "print(f\"Saved filenames to {OUTPUT_FILENAMES_PATH}\")\n",
    "\n",
    "# Optionally, save embeddings and filenames as a CSV\n",
    "print(\"Saving embeddings and filenames to CSV...\")\n",
    "embeddings_df = pd.DataFrame(all_embeddings, columns=[f'embedding_{i}' for i in range(all_embeddings.shape[1])])\n",
    "embeddings_df['filename'] = all_filenames\n",
    "embeddings_df.to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "print(f\"Saved CSV to {OUTPUT_CSV_PATH}\")\n",
    "\n",
    "print(\"Embedding extraction complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ Create embeddings for each resized image (each one with 2048 Dimensions, thanks to ResNet50). Each dimension is in a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the existing embeddings CSV\n",
    "embeddings_df = pd.read_csv(OUTPUT_CSV_PATH)\n",
    "\n",
    "# Create a new DataFrame with two columns: 'filename' and 'embeddings'\n",
    "one_dimensional_df = pd.DataFrame({\n",
    "    'filename': embeddings_df['filename'],\n",
    "    'embeddings': embeddings_df.drop(columns=['filename']).apply(lambda row: row.tolist(), axis=1)\n",
    "})\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "one_dimensional_df.to_csv('oneDimensionalEmbeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ Join all dimensions into 1 single array, leaving 2 columns (one for filename and the other for the embeddings(with 2048 dimensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the 2048D embeddings from CSV\n",
    "df = pd.read_csv('2048DimensionalEmbeddings.csv')\n",
    "\n",
    "# Function to clean up the embedding (from string to numpy array)\n",
    "def clean_embeddings(embedding_str):\n",
    "    return np.array(eval(embedding_str))\n",
    "\n",
    "# Apply PCA and save the reduced embeddings to different files\n",
    "def reduce_and_save(df, n_components, output_file):\n",
    "    # Convert string embeddings to numpy arrays\n",
    "    embeddings = np.array([clean_embeddings(e) for e in df['embeddings']])\n",
    "    \n",
    "    # Apply PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_embeddings = pca.fit_transform(embeddings)\n",
    "    \n",
    "    # Create a DataFrame with reduced embeddings\n",
    "    reduced_df = pd.DataFrame(reduced_embeddings, columns=[f'embeddings_{i+1}' for i in range(n_components)])\n",
    "    \n",
    "    # Add the filename column to the reduced DataFrame\n",
    "    reduced_df['filename'] = df['filename']\n",
    "    \n",
    "    # Save the reduced DataFrame to a CSV file\n",
    "    reduced_df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved {n_components} -dimensional embeddings to {output_file}\")\n",
    "\n",
    "# Reduce to different dimensions and save\n",
    "reduce_and_save(df, 512, '512DimensionalEmbedding.csv')\n",
    "reduce_and_save(df, 256, '256DimensionalEmbedding.csv')\n",
    "reduce_and_save(df, 128, '128DimensionalEmbedding.csv')\n",
    "reduce_and_save(df, 64, '64DimensionalEmbedding.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ Create 4 different versions with less dimensions (for faster processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
